{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb42e9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper_module import whisper\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pdb\n",
    "from datetime import datetime\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pathlib\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import ffmpeg\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81e680c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k66gu\\Documents\\transcription_dev \n",
      " C:\\Users\\k66gu\\Documents\\transcription_dev\\ffmpeg\\ffmpeg.exe \n",
      " C:/Users/k66gu/Documents/transcription_dev/data/input/delta_10.wav  \n",
      " C:\\Users\\k66gu\\Documents\\transcription_dev\\data\\input \n",
      " C:\\Users\\k66gu\\Documents\\transcription_dev\\data\\input\\vad_chunks \n",
      " C:\\Users\\k66gu\\Documents\\transcription_dev\\models\\silero-vad\n"
     ]
    }
   ],
   "source": [
    "basedir = os.getcwd()\n",
    "ffmpeg_path = os.path.join(basedir,\"ffmpeg\",\"ffmpeg.exe\")\n",
    "AUDIO_DIR = os.path.join(basedir, \"data\", \"input\")\n",
    "full_audio_path = 'C:/Users/k66gu/Documents/transcription_dev/data/input/delta_10.wav '# os.path.join(basedir, \"data\",\"input\",\"delta_10.wav\")\n",
    "VAD_DIR = os.path.join(AUDIO_DIR,\"vad_chunks\")\n",
    "SILERO_DIR = os.path.join(basedir, \"models\", \"silero-vad\")\n",
    "print(basedir,'\\n',ffmpeg_path,\"\\n\",full_audio_path,\n",
    "      \"\\n\", AUDIO_DIR,\"\\n\", VAD_DIR, \"\\n\", SILERO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14ab9620",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transcriber(object):\n",
    "    \"\"\"\n",
    "    This class implements a new VAD ontop of Whisper Transcriber\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.VAD_THRESHOLD = 0.4 # Confidence threshold for VAD speech vs non-speech\n",
    "        self.VAD_SR = 16000 # Sample rate to resample to\n",
    "        self.head = 3200 # 0.2s head for padding chunks\n",
    "        self.tail = 20800 # 1.32 tail for padding chunks\n",
    "        self.chunk_threshold = 3.0 \n",
    "\n",
    "        # Pass in directories by unpacking a list\n",
    "        # List order should be in row order below\n",
    "        self.SILERO_DIR = args[0] # This should be passed in from os.path.dirname(sys.argv[0]) or os.getcwd() if dev\n",
    "        self.full_audio_path = args[1] # This will get passed in from a button or another class attribute\n",
    "        self.AUDIO_DIR = args[2]\n",
    "\n",
    "    def _vad_dir_creator(self):\n",
    "        \"\"\"\n",
    "        Create directory for VAD chunks if not existing already\n",
    "        \"\"\"\n",
    "        AUDIO_DIR = self.AUDIO_DIR\n",
    "        if not os.path.exists(os.path.join(AUDIO_DIR,\"vad_chunks\")):\n",
    "            print(\"Creating vad chunks directory...\")\n",
    "            os.mkdir(os.path.join(AUDIO_DIR,\"vad_chunks\"))\n",
    "        \n",
    "        # Store directory as we need somewhere to look for later on\n",
    "        self.VAD_DIR = os.path.join(AUDIO_DIR, \"vad_chunks\")\n",
    "        print(\"Directory created!\")\n",
    "        \n",
    "    def _create_temp_audio(self, ffmpeg_path: str):\n",
    "        \"\"\"\n",
    "        Create a copy of the input audio from full_audio_path and write to a temp directory VAD_DIR\n",
    "        \n",
    "        Args:\n",
    "            ffmpeg_path (str): path to ffmpeg executable\n",
    "        \"\"\"\n",
    "        full_audio_path = self.full_audio_path\n",
    "        VAD_DIR = self.VAD_DIR\n",
    "        # Hardcode values to feed into ffmpeg\n",
    "        # This is just a temporary copy which requires specific formatted values to ensure\n",
    "        # that we get the appropriate input to the system\n",
    "        ffmpeg.input(full_audio_path).output(\n",
    "            VAD_DIR+\"/vad_temp.wav\",\n",
    "            ar=\"16000\",\n",
    "            ac=\"1\",\n",
    "            acodec=\"pcm_s16le\",\n",
    "            map_metadata=\"-1\",\n",
    "            fflags=\"+bitexact\",\n",
    "        ).overwrite_output().run(cmd=[ffmpeg_path, \"-nostdin\"], capture_stdout=True, capture_stderr=True, quiet=True)\n",
    "        print(os.path.exists(os.path.join(VAD_DIR,\"vad_temp.wav\")))\n",
    "    \n",
    "    def _load_vad(self):\n",
    "        \"\"\"\n",
    "        Load the VAD model from local\n",
    "        Store utils from Silero as methods within the transcriber class\n",
    "        \"\"\"\n",
    "        vad_model, utils = torch.hub._load_local(\n",
    "            hubconf_dir=self.SILERO_DIR, model=\"silero_vad_local\", onnx=False\n",
    "        )\n",
    "        if (vad_model is not None) and (utils is not None):\n",
    "            print(\"Model and utils loaded!\")\n",
    "        print(utils)\n",
    "\n",
    "        # Store the functions within the transcriber object for easy access\n",
    "        (self.get_speech_timestamps, self.save_audio, self.read_audio, \n",
    "         self.load_audio, self.VADIterator, self.collect_chunks) = utils\n",
    "\n",
    "        self.vad_model = vad_model \n",
    "    \n",
    "    def _read_audio_old(self):\n",
    "        \"\"\"\n",
    "        Read audio into an attribute to use\n",
    "        The path here will be hardcoded as everything should be self contained\n",
    "        The self-containment is from the directory and file copying from\n",
    "        _vad_dir_creator() and _create_temp_audio() which if properly run\n",
    "        will result in absolute file locations    \n",
    "        \"\"\"\n",
    "        self.wav_old = self.read_audio(\"data/input/vad_chunks/vad_temp.wav\", sampling_rate=self.VAD_SR)\n",
    "        print(\"Audio loaded! at %s\" % (\"data/input/vad_chunks/vad_temp.wav\"))\n",
    "        \n",
    "    def _read_audio(self, ffmpeg_path:str):\n",
    "            \"\"\"\n",
    "            Read audio into an attribute to use\n",
    "            The path here will be hardcoded as everything should be self contained\n",
    "            The self-containment is from the directory and file copying from\n",
    "            _vad_dir_creator() and _create_temp_audio() which if properly run\n",
    "            will result in absolute file locations    \n",
    "\n",
    "            Args:\n",
    "                ffmpeg_path (str): path to ffmpeg executable\n",
    "            \"\"\"\n",
    "            # Hardcode the location of the copied temporary wav\n",
    "            # This is read in as a np_buffer which we convert to a tensor using torch\n",
    "            audio = \"data/input/vad_chunks/vad_temp.wav\"\n",
    "            if not torch.is_tensor(audio):\n",
    "                if isinstance(audio, str):\n",
    "                    audio = self.load_audio(audio, ffmpeg_path,sr=self.VAD_SR)\n",
    "                audio = torch.from_numpy(audio)\n",
    "            self.wav = audio\n",
    "            print(\"Audio loaded! at %s\" % (\"data/input/vad_chunks/vad_temp.wav\"))\n",
    "    \n",
    "    \n",
    "    def _process_timestamps(self):\n",
    "        \"\"\"\n",
    "        Add padding, remove small gaps and overlaps from processed audio\n",
    "        Result timestamps are in samples (not seconds) \n",
    "        They represent chunks of audio\n",
    "        \"\"\"\n",
    "        t = self.get_speech_timestamps(self.wav, self.vad_model, sampling_rate=self.VAD_SR, threshold=self.VAD_THRESHOLD)    \n",
    "        # Add a bit of padding, and remove small gaps\n",
    "        for i in range(len(t)):\n",
    "            t[i][\"start\"] = max(0, t[i][\"start\"] - self.head)  # 0.2s head -> self.head=3200\n",
    "            t[i][\"end\"] = min(self.wav.shape[0] - 16, t[i][\"end\"] + self.tail)  # 1.3s tail -> self.tail = 20800\n",
    "            if i > 0 and t[i][\"start\"] < t[i - 1][\"end\"]:\n",
    "                t[i][\"start\"] = t[i - 1][\"end\"]  # Remove overlap\n",
    "        self.timestamps = t # Store timestamps to edit\n",
    "        \n",
    "        print(\"Timestamps processed! No. of audio chunks are: {}\".format(len(self.timestamps)))\n",
    "        \n",
    "    def _split_audio(self): \n",
    "        \"\"\"\n",
    "        If breaks are longer than chunk_threshold seconds, \n",
    "            split into a new audio file\n",
    "        This'll effectively turn long transcriptions into many shorter ones\n",
    "        \n",
    "        \"\"\"\n",
    "        # Store the chunked audio into a matrix\n",
    "        # Each row is each chunk\n",
    "        u = [[]]\n",
    "        for i in range(len(self.timestamps)):\n",
    "            if i > 0 and self.timestamps[i][\"start\"] > self.timestamps[i - 1][\"end\"] + (self.chunk_threshold * self.VAD_SR):\n",
    "                u.append([])\n",
    "            u[-1].append(self.timestamps[i])\n",
    "        self.chunked_audio = u # Store the matrix of chunked audio\n",
    "        \n",
    "        print(\"No. of chunked audio based on threshold: {}\".format(len(self.chunked_audio)))\n",
    "    \n",
    "    def _merge_chunks(self):\n",
    "        \"\"\"\n",
    "        Merge chunks and remove temp copy of original audio\n",
    "        Save chunks to a directory\n",
    "\n",
    "        \"\"\"\n",
    "        for i in range(len(self.chunked_audio)):\n",
    "            self.save_audio(\n",
    "                \"data/input/vad_chunks/\" + str(i) + \".wav\", # Fix the hardcoded locations for path\n",
    "                self.collect_chunks(self.chunked_audio[i], self.wav),\n",
    "                sampling_rate=self.VAD_SR,\n",
    "            )\n",
    "        if len(os.listdir(\"data/input/vad_chunks\")) != 0:\n",
    "            print(\"Audio chunks saved!\")\n",
    "        os.remove(\"data/input/vad_chunks/vad_temp.wav\") # Fix hardcoded paths\n",
    "    \n",
    "    def _convert_timestamps_seconds(self):\n",
    "        \"\"\"\n",
    "        Convert timestamps into seconds format\n",
    "        \"\"\"\n",
    "        # Go through each individual chunked audio within the matrix\n",
    "        # Identify chunks and offsets for the audio through math and using 16000 SR\n",
    "        # keys: start, end, chunk_start and chunk_end should be present if previous executions worked\n",
    "        for i in range(len(self.chunked_audio)):\n",
    "            time = 0.0\n",
    "            offset = 0.0\n",
    "            for j in range(len(self.chunked_audio[i])):\n",
    "                self.chunked_audio[i][j][\"start\"] /= self.VAD_SR\n",
    "                self.chunked_audio[i][j][\"end\"] /= self.VAD_SR\n",
    "                self.chunked_audio[i][j][\"chunk_start\"] = time\n",
    "                time += self.chunked_audio[i][j][\"end\"] - self.chunked_audio[i][j][\"start\"]\n",
    "                self.chunked_audio[i][j][\"chunk_end\"] = time\n",
    "                if j == 0:\n",
    "                    offset += self.chunked_audio[i][j][\"start\"]\n",
    "                else:\n",
    "                    offset += self.chunked_audio[i][j][\"start\"] - self.chunked_audio[i][j - 1][\"end\"]\n",
    "                self.chunked_audio[i][j][\"offset\"] = offset\n",
    "        print(\"Timestamps converted!\")\n",
    "    \n",
    "    def _whisper_on_chunks(self, ffmpeg_path:str, basedir: str, model_name:str = 'medium'):\n",
    "        \"\"\"\n",
    "        Transcribe using whisper on the pre-processed chunked audio\n",
    "        Whisper using the medium model\n",
    "\n",
    "        Args:\n",
    "            ffmpeg_path (str): path to ffmpeg executable e.g., os.path.join(basedir,\"ffmpeg\",\"ffmpeg.exe\")\n",
    "            basedir (str): Path where executable should be located e.g., os.getcwd() \n",
    "            model_name (str): Default \"medium\" for Whisper model size\n",
    "        \"\"\"\n",
    "        # Load the whisper model\n",
    "        model = whisper.load_model_local(f\"{model_name}.en\", basedir ,in_memory=True)\n",
    "        task = 'transcribe'\n",
    "        language = 'english'\n",
    "        initial_prompt = ''\n",
    "\n",
    "        # Transcribe each chunk using Whisper\n",
    "        for i in tqdm(range(len(self.chunked_audio))):\n",
    "            result = model.transcribe(\n",
    "                os.path.join(self.VAD_DIR,str(i) + \".wav\" ), ffmpeg_path=ffmpeg_path, \n",
    "                task=task, language=language, initial_prompt=initial_prompt\n",
    "            )\n",
    "            # Break if result doesn't end with severe hallucinations\n",
    "            if len(result[\"segments\"]) == 0:\n",
    "                break\n",
    "            elif result[\"segments\"][-1][\"end\"] < self.chunked_audio[i][-1][\"chunk_end\"] + 10.0:\n",
    "                break\n",
    "        self.result = result\n",
    "        if len(result) > 0:\n",
    "            print(\"Audio successfully transcribed!\")\n",
    "    \n",
    "    def _run(self, ffmpeg_path:str, basedir: str):\n",
    "        \"\"\"\n",
    "        This method executes all the required steps required for Whisper\n",
    "        \"\"\"\n",
    "        self._vad_dir_creator()\n",
    "        self._create_temp_audio()\n",
    "        self._load_vad()\n",
    "        self._read_audio()\n",
    "        self._process_timestamps()\n",
    "        self._split_audio()\n",
    "        self._merge_chunks()\n",
    "        self._convert_timestamps_seconds()\n",
    "        self._whisper_on_chunks(ffmpeg_path = ffmpeg_path, basedir = basedir)\n",
    "            \n",
    "    def whisper_csv(self, ffmpeg_path:str, basedir: str):\n",
    "        \"\"\"\n",
    "        Seperate whisper results into csv only\n",
    "        \"\"\"\n",
    "        self._run(ffmpeg_path = ffmpeg_path, basedir = basedir) # Execute everything needed\n",
    "        segment_info = []\n",
    "        result = self.result\n",
    "        \n",
    "        for i in tqdm(range(len(self.chunked_audio))):\n",
    "            for r in result[\"segments\"]:\n",
    "                # Skip audio timestamped after the chunk has ended\n",
    "                if r[\"start\"] > self.chunked_audio[i][-1][\"chunk_end\"]:\n",
    "                    continue\n",
    "                # Keep segment info for debugging\n",
    "                # segment_info.append(r)\n",
    "\n",
    "                \n",
    "                # Skip if log prob is low or no speech prob is high\n",
    "                if r[\"avg_logprob\"] < -1.0 or r[\"no_speech_prob\"] > 0.7: # Hardcoded thresholds\n",
    "                    continue\n",
    "                # Set start timestamp\n",
    "                start = r[\"start\"] + self.chunked_audio[i][0][\"offset\"]\n",
    "                for j in range(len(self.chunked_audio[i])):\n",
    "                    if (\n",
    "                        r[\"start\"] >= self.chunked_audio[i][j][\"chunk_start\"]\n",
    "                        and r[\"start\"] <= self.chunked_audio[i][j][\"chunk_end\"]\n",
    "                    ):\n",
    "                        start = r[\"start\"] + self.chunked_audio[i][j][\"offset\"]\n",
    "                        break\n",
    "                end = self.chunked_audio[i][-1][\"end\"] + 0.5\n",
    "                for j in range(len(self.chunked_audio[i])):\n",
    "                    if r[\"end\"] >= self.chunked_audio[i][j][\"chunk_start\"] and r[\"end\"] <= self.chunked_audio[i][j][\"chunk_end\"]:\n",
    "                        end = r[\"end\"] + self.chunked_audio[i][j][\"offset\"]\n",
    "                        break\n",
    "                segment_info.append(r)\n",
    "        return segment_info\n",
    "    \n",
    "    def whisper_text(self, ffmpeg_path: str, basedir: str):\n",
    "        \"\"\"\n",
    "        Separate the whisper results into text for output\n",
    "        \"\"\"\n",
    "        self._run(ffmpeg_path = ffmpeg_path, basedir = basedir)\n",
    "        text_info = []\n",
    "        result = self.result\n",
    "\n",
    "        # Post process the segments and text based using the offsets and \n",
    "        # identified chunk start and end to ensure we can remove overlaps\n",
    "        for i in tqdm(range(len(self.chunked_audio))):\n",
    "            for r in result[\"segments\"]:\n",
    "                # Skip audio timestamped after the chunk has ended\n",
    "                if r[\"start\"] > self.chunked_audio[i][-1][\"chunk_end\"]:\n",
    "                    continue\n",
    "                # Skip if log prob is low or no speech prob is high\n",
    "                if r[\"avg_logprob\"] < -1.0 or r[\"no_speech_prob\"] > 0.7: # Hardcoded thresholds\n",
    "                    continue\n",
    "                # Set start timestamp\n",
    "                start = r[\"start\"] + self.chunked_audio[i][0][\"offset\"]\n",
    "                for j in range(len(self.chunked_audio[i])):\n",
    "                    if (\n",
    "                        r[\"start\"] >= self.chunked_audio[i][j][\"chunk_start\"]\n",
    "                        and r[\"start\"] <= self.chunked_audio[i][j][\"chunk_end\"]\n",
    "                    ):\n",
    "                        start = r[\"start\"] + self.chunked_audio[i][j][\"offset\"]\n",
    "                        break\n",
    "                end = self.chunked_audio[i][-1][\"end\"] + 0.5\n",
    "                for j in range(len(self.chunked_audio[i])):\n",
    "                    if r[\"end\"] >= self.chunked_audio[i][j][\"chunk_start\"] and r[\"end\"] <= self.chunked_audio[i][j][\"chunk_end\"]:\n",
    "                        end = r[\"end\"] + self.chunked_audio[i][j][\"offset\"]\n",
    "                        break\n",
    "                \n",
    "                text_info.append(r[\"text\"].strip())\n",
    "        return text_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a973c29b",
   "metadata": {},
   "source": [
    "# Unit & Integration testing\n",
    "* Unit testing row by row to ensure it works\n",
    "* Integration testing row by row as each subsequent method requires the previous row to be executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "657e99ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber = Transcriber(*[SILERO_DIR, full_audio_path, AUDIO_DIR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae69e8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created!\n"
     ]
    }
   ],
   "source": [
    "transcriber._vad_dir_creator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d999c242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "transcriber._create_temp_audio(ffmpeg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23043b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and utils loaded!\n",
      "(<function get_speech_timestamps at 0x000002D23806FA60>, <function save_audio at 0x000002D23806F8B0>, <function read_audio at 0x000002D237D23DC0>, <function load_audio at 0x000002D23806F820>, <class 'utils_vad.VADIterator'>, <function collect_chunks at 0x000002D23806FCA0>)\n"
     ]
    }
   ],
   "source": [
    "transcriber._load_vad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba07b47",
   "metadata": {},
   "source": [
    "#### Compare Torchaudio load vs FFMPEG load\n",
    "* wav_old should equal wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa35cecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio loaded! at data/input/vad_chunks/vad_temp.wav\n",
      "wav file tensor: tensor([-2.1362e-04, -2.1362e-04, -3.0518e-05,  2.4414e-04,  2.4414e-04])\n"
     ]
    }
   ],
   "source": [
    "transcriber._read_audio_old()\n",
    "print(\"wav file tensor: {}\".format(transcriber.wav_old[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b94fa93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio loaded! at data/input/vad_chunks/vad_temp.wav\n",
      "wav file tensor: tensor([-2.1362e-04, -2.1362e-04, -3.0518e-05,  2.4414e-04,  2.4414e-04])\n"
     ]
    }
   ],
   "source": [
    "transcriber._read_audio(ffmpeg_path)\n",
    "print(\"wav file tensor: {}\".format(transcriber.wav[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fa5245c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFMPEG method and torchaudio method result in same input audio tensor!\n"
     ]
    }
   ],
   "source": [
    "# Check if our inputs are the same\n",
    "if all(torch.eq(transcriber.wav, transcriber.wav_old)):\n",
    "    print(\"FFMPEG method and torchaudio method result in same input audio tensor!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b63c1854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamps processed! No. of audio chunks are: 3\n",
      "Timestamps: [{'start': 5024, 'end': 56096}, {'start': 56096, 'end': 135456}, {'start': 135456, 'end': 159984}]\n"
     ]
    }
   ],
   "source": [
    "transcriber._process_timestamps()\n",
    "print(\"Timestamps: {}\".format(transcriber.timestamps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd9d0732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of chunked audio based on threshold: 1\n",
      "Audio chunks: [[{'start': 5024, 'end': 56096}, {'start': 56096, 'end': 135456}, {'start': 135456, 'end': 159984}]]\n"
     ]
    }
   ],
   "source": [
    "transcriber._split_audio()\n",
    "print(\"Audio chunks: {}\".format(transcriber.chunked_audio))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2402706b",
   "metadata": {},
   "source": [
    "#### Compare FFMPEG save vs Torchaudio Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "984c4bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'start': 5024, 'end': 56096},\n",
       "  {'start': 56096, 'end': 135456},\n",
       "  {'start': 135456, 'end': 159984}]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_wav = transcriber.chunked_audio\n",
    "save_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05909947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.1553e-05, -9.1553e-05, -1.2207e-04,  ..., -4.2114e-03,\n",
       "        -6.1035e-04,  5.0659e-03])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_wav_2 = transcriber.collect_chunks(save_wav[0], transcriber.wav)\n",
    "save_wav_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a7f87ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.1553e-05, -9.1553e-05, -1.2207e-04,  ..., -4.2114e-03,\n",
       "         -6.1035e-04,  5.0659e-03]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_wav_2.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44a4a788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_wav_2.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2470ab87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([154960])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_wav_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3c6195d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dc05ff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_subtype(dtype: torch.dtype, format: str, encoding: str, bits_per_sample: int):\n",
    "    if format == \"wav\":\n",
    "        return _get_subtype_for_wav(dtype, encoding, bits_per_sample)\n",
    "    if format == \"flac\":\n",
    "        if encoding:\n",
    "            raise ValueError(\"flac does not support encoding.\")\n",
    "        if not bits_per_sample:\n",
    "            return \"PCM_16\"\n",
    "        if bits_per_sample > 24:\n",
    "            raise ValueError(\"flac does not support bits_per_sample > 24.\")\n",
    "        return \"PCM_S8\" if bits_per_sample == 8 else f\"PCM_{bits_per_sample}\"\n",
    "    if format in (\"ogg\", \"vorbis\"):\n",
    "        if encoding or bits_per_sample:\n",
    "            raise ValueError(\"ogg/vorbis does not support encoding/bits_per_sample.\")\n",
    "        return \"VORBIS\"\n",
    "    if format == \"sph\":\n",
    "        return _get_subtype_for_sphere(encoding, bits_per_sample)\n",
    "    if format in (\"nis\", \"nist\"):\n",
    "        return \"PCM_16\"\n",
    "    raise ValueError(f\"Unsupported format: {format}\")\n",
    "\n",
    "def _get_subtype_for_wav(dtype: torch.dtype, encoding: str, bits_per_sample: int):\n",
    "    if not encoding:\n",
    "        if not bits_per_sample:\n",
    "            subtype = {\n",
    "                torch.uint8: \"PCM_U8\",\n",
    "                torch.int16: \"PCM_16\",\n",
    "                torch.int32: \"PCM_32\",\n",
    "                torch.float32: \"FLOAT\",\n",
    "                torch.float64: \"DOUBLE\",\n",
    "            }.get(dtype)\n",
    "            if not subtype:\n",
    "                raise ValueError(f\"Unsupported dtype for wav: {dtype}\")\n",
    "            return subtype\n",
    "        if bits_per_sample == 8:\n",
    "            return \"PCM_U8\"\n",
    "        return f\"PCM_{bits_per_sample}\"\n",
    "    if encoding == \"PCM_S\":\n",
    "        if not bits_per_sample:\n",
    "            return \"PCM_32\"\n",
    "        if bits_per_sample == 8:\n",
    "            raise ValueError(\"wav does not support 8-bit signed PCM encoding.\")\n",
    "        return f\"PCM_{bits_per_sample}\"\n",
    "    if encoding == \"PCM_U\":\n",
    "        if bits_per_sample in (None, 8):\n",
    "            return \"PCM_U8\"\n",
    "        raise ValueError(\"wav only supports 8-bit unsigned PCM encoding.\")\n",
    "    if encoding == \"PCM_F\":\n",
    "        if bits_per_sample in (None, 32):\n",
    "            return \"FLOAT\"\n",
    "        if bits_per_sample == 64:\n",
    "            return \"DOUBLE\"\n",
    "        raise ValueError(\"wav only supports 32/64-bit float PCM encoding.\")\n",
    "    if encoding == \"ULAW\":\n",
    "        if bits_per_sample in (None, 8):\n",
    "            return \"ULAW\"\n",
    "        raise ValueError(\"wav only supports 8-bit mu-law encoding.\")\n",
    "    if encoding == \"ALAW\":\n",
    "        if bits_per_sample in (None, 8):\n",
    "            return \"ALAW\"\n",
    "        raise ValueError(\"wav only supports 8-bit a-law encoding.\")\n",
    "    raise ValueError(f\"wav does not support {encoding}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "24a0c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_subtype(dtype: torch.dtype, format: str, encoding: str, bits_per_sample: int):\n",
    "    if format == \"wav\":\n",
    "        return _get_subtype_for_wav(dtype, encoding, bits_per_sample)\n",
    "    if format == \"flac\":\n",
    "        if encoding:\n",
    "            raise ValueError(\"flac does not support encoding.\")\n",
    "        if not bits_per_sample:\n",
    "            return \"PCM_16\"\n",
    "        if bits_per_sample > 24:\n",
    "            raise ValueError(\"flac does not support bits_per_sample > 24.\")\n",
    "        return \"PCM_S8\" if bits_per_sample == 8 else f\"PCM_{bits_per_sample}\"\n",
    "    if format in (\"ogg\", \"vorbis\"):\n",
    "        if encoding or bits_per_sample:\n",
    "            raise ValueError(\"ogg/vorbis does not support encoding/bits_per_sample.\")\n",
    "        return \"VORBIS\"\n",
    "    if format == \"sph\":\n",
    "        return _get_subtype_for_sphere(encoding, bits_per_sample)\n",
    "    if format in (\"nis\", \"nist\"):\n",
    "        return \"PCM_16\"\n",
    "    raise ValueError(f\"Unsupported format: {format}\")\n",
    "\n",
    "def _get_subtype_for_wav(dtype: torch.dtype, encoding: str, bits_per_sample: int):\n",
    "    if not encoding:\n",
    "        if not bits_per_sample:\n",
    "            subtype = {\n",
    "                torch.uint8: \"PCM_U8\",\n",
    "                torch.int16: \"PCM_16\",\n",
    "                torch.int32: \"PCM_32\",\n",
    "                torch.float32: \"FLOAT\",\n",
    "                torch.float64: \"DOUBLE\",\n",
    "            }.get(dtype)\n",
    "            if not subtype:\n",
    "                raise ValueError(f\"Unsupported dtype for wav: {dtype}\")\n",
    "            return subtype\n",
    "        if bits_per_sample == 8:\n",
    "            return \"PCM_U8\"\n",
    "        return f\"PCM_{bits_per_sample}\"\n",
    "    if encoding == \"PCM_S\":\n",
    "        if not bits_per_sample:\n",
    "            return \"PCM_32\"\n",
    "        if bits_per_sample == 8:\n",
    "            raise ValueError(\"wav does not support 8-bit signed PCM encoding.\")\n",
    "        return f\"PCM_{bits_per_sample}\"\n",
    "    if encoding == \"PCM_U\":\n",
    "        if bits_per_sample in (None, 8):\n",
    "            return \"PCM_U8\"\n",
    "        raise ValueError(\"wav only supports 8-bit unsigned PCM encoding.\")\n",
    "    if encoding == \"PCM_F\":\n",
    "        if bits_per_sample in (None, 32):\n",
    "            return \"FLOAT\"\n",
    "        if bits_per_sample == 64:\n",
    "            return \"DOUBLE\"\n",
    "        raise ValueError(\"wav only supports 32/64-bit float PCM encoding.\")\n",
    "    if encoding == \"ULAW\":\n",
    "        if bits_per_sample in (None, 8):\n",
    "            return \"ULAW\"\n",
    "        raise ValueError(\"wav only supports 8-bit mu-law encoding.\")\n",
    "    if encoding == \"ALAW\":\n",
    "        if bits_per_sample in (None, 8):\n",
    "            return \"ALAW\"\n",
    "        raise ValueError(\"wav only supports 8-bit a-law encoding.\")\n",
    "    raise ValueError(f\"wav does not support {encoding}.\")\n",
    "\n",
    "def save(\n",
    "    filepath: str,\n",
    "    src: torch.Tensor,\n",
    "    sample_rate: int,\n",
    "    channels_first: bool = True,\n",
    "    compression: [float] = None,\n",
    "    format: [str] = None,\n",
    "    encoding:[str] = None,\n",
    "    bits_per_sample: [int] = None,\n",
    "):\n",
    "    \"\"\"Save audio data to file.\n",
    "\n",
    "    Note:\n",
    "        The formats this function can handle depend on the soundfile installation.\n",
    "        This function is tested on the following formats;\n",
    "\n",
    "        * WAV\n",
    "\n",
    "            * 32-bit floating-point\n",
    "            * 32-bit signed integer\n",
    "            * 16-bit signed integer\n",
    "            * 8-bit unsigned integer\n",
    "\n",
    "        * FLAC\n",
    "        * OGG/VORBIS\n",
    "        * SPHERE\n",
    "\n",
    "    Note:\n",
    "        ``filepath`` argument is intentionally annotated as ``str`` only, even though it accepts\n",
    "        ``pathlib.Path`` object as well. This is for the consistency with ``\"sox_io\"`` backend,\n",
    "        which has a restriction on type annotation due to TorchScript compiler compatiblity.\n",
    "\n",
    "    Args:\n",
    "        filepath (str or pathlib.Path): Path to audio file.\n",
    "        src (torch.Tensor): Audio data to save. must be 2D tensor.\n",
    "        sample_rate (int): sampling rate\n",
    "        channels_first (bool, optional): If ``True``, the given tensor is interpreted as `[channel, time]`,\n",
    "            otherwise `[time, channel]`.\n",
    "        compression (float of None, optional): Not used.\n",
    "            It is here only for interface compatibility reson with \"sox_io\" backend.\n",
    "        format (str or None, optional): Override the audio format.\n",
    "            When ``filepath`` argument is path-like object, audio format is\n",
    "            inferred from file extension. If the file extension is missing or\n",
    "            different, you can specify the correct format with this argument.\n",
    "\n",
    "            When ``filepath`` argument is file-like object,\n",
    "            this argument is required.\n",
    "\n",
    "            Valid values are ``\"wav\"``, ``\"ogg\"``, ``\"vorbis\"``,\n",
    "            ``\"flac\"`` and ``\"sph\"``.\n",
    "        encoding (str or None, optional): Changes the encoding for supported formats.\n",
    "            This argument is effective only for supported formats, sush as\n",
    "            ``\"wav\"``, ``\"\"flac\"`` and ``\"sph\"``. Valid values are;\n",
    "\n",
    "                - ``\"PCM_S\"`` (signed integer Linear PCM)\n",
    "                - ``\"PCM_U\"`` (unsigned integer Linear PCM)\n",
    "                - ``\"PCM_F\"`` (floating point PCM)\n",
    "                - ``\"ULAW\"`` (mu-law)\n",
    "                - ``\"ALAW\"`` (a-law)\n",
    "\n",
    "        bits_per_sample (int or None, optional): Changes the bit depth for the\n",
    "            supported formats.\n",
    "            When ``format`` is one of ``\"wav\"``, ``\"flac\"`` or ``\"sph\"``,\n",
    "            you can change the bit depth.\n",
    "            Valid values are ``8``, ``16``, ``24``, ``32`` and ``64``.\n",
    "\n",
    "    Supported formats/encodings/bit depth/compression are:\n",
    "\n",
    "    ``\"wav\"``\n",
    "        - 32-bit floating-point PCM\n",
    "        - 32-bit signed integer PCM\n",
    "        - 24-bit signed integer PCM\n",
    "        - 16-bit signed integer PCM\n",
    "        - 8-bit unsigned integer PCM\n",
    "        - 8-bit mu-law\n",
    "        - 8-bit a-law\n",
    "\n",
    "        Note:\n",
    "            Default encoding/bit depth is determined by the dtype of\n",
    "            the input Tensor.\n",
    "\n",
    "    ``\"flac\"``\n",
    "        - 8-bit\n",
    "        - 16-bit (default)\n",
    "        - 24-bit\n",
    "\n",
    "    ``\"ogg\"``, ``\"vorbis\"``\n",
    "        - Doesn't accept changing configuration.\n",
    "\n",
    "    ``\"sph\"``\n",
    "        - 8-bit signed integer PCM\n",
    "        - 16-bit signed integer PCM\n",
    "        - 24-bit signed integer PCM\n",
    "        - 32-bit signed integer PCM (default)\n",
    "        - 8-bit mu-law\n",
    "        - 8-bit a-law\n",
    "        - 16-bit a-law\n",
    "        - 24-bit a-law\n",
    "        - 32-bit a-law\n",
    "\n",
    "    \"\"\"\n",
    "    if src.ndim != 2:\n",
    "        raise ValueError(f\"Expected 2D Tensor, got {src.ndim}D.\")\n",
    "    if compression is not None:\n",
    "        warnings.warn(\n",
    "            '`save` function of \"soundfile\" backend does not support \"compression\" parameter. '\n",
    "            \"The argument is silently ignored.\"\n",
    "        )\n",
    "    if hasattr(filepath, \"write\"):\n",
    "        if format is None:\n",
    "            raise RuntimeError(\"`format` is required when saving to file object.\")\n",
    "        ext = format.lower()\n",
    "    else:\n",
    "        ext = str(filepath).split(\".\")[-1].lower()\n",
    "\n",
    "    if bits_per_sample not in (None, 8, 16, 24, 32, 64):\n",
    "        raise ValueError(\"Invalid bits_per_sample.\")\n",
    "    if bits_per_sample == 24:\n",
    "        warnings.warn(\n",
    "            \"Saving audio with 24 bits per sample might warp samples near -1. \"\n",
    "            \"Using 16 bits per sample might be able to avoid this.\"\n",
    "        )\n",
    "    subtype = _get_subtype(src.dtype, ext, encoding, bits_per_sample)\n",
    "\n",
    "    # sph is a extension used in TED-LIUM but soundfile does not recognize it as NIST format,\n",
    "    # so we extend the extensions manually here\n",
    "    if ext in [\"nis\", \"nist\", \"sph\"] and format is None:\n",
    "        format = \"NIST\"\n",
    "\n",
    "    if channels_first:\n",
    "        src = src.t()\n",
    "\n",
    "    soundfile.write(file=filepath, data=src, samplerate=sample_rate, subtype=subtype, format=format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7bc755a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"test.wav\"\n",
    "tensor_to_save = save_wav_2\n",
    "sampling_rate = 16000\n",
    "save(path, tensor_to_save.unsqueeze(0), sampling_rate, bits_per_sample=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1c3eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "dffb0cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio chunks saved!\n"
     ]
    }
   ],
   "source": [
    "transcriber._merge_chunks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "d6fc6bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamps converted!\n"
     ]
    }
   ],
   "source": [
    "transcriber._convert_timestamps_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "fc66768c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/1 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio successfully transcribed!\n"
     ]
    }
   ],
   "source": [
    "# basedir = os.getcwd()\n",
    "# ffmpeg_path = os.path.join(basedir,\"ffmpeg\",\"ffmpeg.exe\")\n",
    "transcriber._whisper_on_chunks(ffmpeg_path = ffmpeg_path, basedir = basedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "210d9542",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Delta 2846, the fire department would like to know are you expecting or would you like an inspection when you get off the runway?'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcriber.result['text'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "0d082b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "segments = transcriber._whisper_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "4f0c976a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1004.62it/s]\n"
     ]
    }
   ],
   "source": [
    "text_only = transcriber._whisper_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ef32f4c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'seek': 0,\n",
       "  'start': 0.0,\n",
       "  'end': 9.68,\n",
       "  'text': ' Delta 2846, the fire department would like to know are you expecting or would you like an inspection when you get off the runway?',\n",
       "  'tokens': [50363,\n",
       "   16978,\n",
       "   2579,\n",
       "   3510,\n",
       "   11,\n",
       "   262,\n",
       "   2046,\n",
       "   5011,\n",
       "   561,\n",
       "   588,\n",
       "   284,\n",
       "   760,\n",
       "   389,\n",
       "   345,\n",
       "   12451,\n",
       "   393,\n",
       "   561,\n",
       "   345,\n",
       "   588,\n",
       "   281,\n",
       "   15210,\n",
       "   618,\n",
       "   345,\n",
       "   651,\n",
       "   572,\n",
       "   262,\n",
       "   23443,\n",
       "   30,\n",
       "   50847],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.34617840449015297,\n",
       "  'compression_ratio': 1.2169811320754718,\n",
       "  'no_speech_prob': 0.05624688044190407}]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "82922f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Delta 2846, the fire department would like to know are you expecting or would you like an inspection when you get off the runway?']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "8d817335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created!\n",
      "True\n",
      "Model and utils loaded!\n",
      "Audio loaded! at data/input/vad_chunks/vad_temp.wav\n",
      "Timestamps processed! No. of audio chunks are: 3\n",
      "No. of chunked audio based on threshold: 1\n",
      "Audio chunks saved!\n",
      "Timestamps converted!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/1 [00:14<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio successfully transcribed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "segments_info = transcriber.whisper_csv(ffmpeg_path = ffmpeg_path, basedir = basedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "1c667fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert transcriber.result['segments'] == segments_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "ad617839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created!\n",
      "True\n",
      "Model and utils loaded!\n",
      "Audio loaded! at data/input/vad_chunks/vad_temp.wav\n",
      "Timestamps processed! No. of audio chunks are: 3\n",
      "No. of chunked audio based on threshold: 1\n",
      "Audio chunks saved!\n",
      "Timestamps converted!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/1 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio successfully transcribed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "text_info = transcriber.whisper_text(ffmpeg_path = ffmpeg_path, basedir = basedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "5a023f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert text_info[0] == transcriber.result['text'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f3fd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79df127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2602c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95a743d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f575cd64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transcription",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:50:36) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "ff9cd90a0e30e33d15f1302046eb88ea53f0838664e0d5e800ad052c34deac05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
